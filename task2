语言模型的监督式微调（SFT）
概述
  概念：通过最小化目标回复的负对数似然，使模型学会模仿期望的行为并在面对提示时做出合适回应。
  特点：是语言模型对齐的重要基础方法。
  核心：让基础模型根据提示预测下一个标记，学会生成预期理想回答。
  流程：
    基础模型：未经调整的模型给不出准确的回答，大多数情况下给出泛泛或重复的回应，例如反问。
    带标签的数据集：收集并整理用户提供的指令-响应对数据。
    SFT训练：对指令-响应对数据集进行微调，通过最小化回应的交叉熵损失来训练模型，最大化在每个提示条件下生成理想目标回应的联合概率。
    SFT训练（微调）后的模型：模型可以针对指令给出合适的响应。
  SFT本质：模型模仿学习。
最佳的使用场景是激发模型新行为和提升模型能力时。
  激发模型新行为：
    将预训练模型转变为能遵守指令的模型。
    让基础模型学会基本推理。
    使模型在没有明确说明的情况下使用特定工具。
  提升模型能力：
    利用强大的大模型生成高质量合成数据，通过训练把这些能力“蒸馏”到小模型中，即从大模型向小模型“蒸馏”能力。
  SFT通常在预训练和更高级对齐方法间起桥梁作用，如果需要模型快速适应新行为并有示例数据时，推荐尝试此种方法。
SFT 数据策划原则
  训练效果高度依赖数据集的质量，优质数据能让模型学会有用行为，劣质数据会让模型模仿不良习惯。
  常用的数据策划方法：
    蒸馏：用更强的模型对指令生成回复，训练小模型去模仿这些回复，把强模型的能力迁移到弱模型上。
    Best‑of‑K / 拒绝采样：针对同一提示生成多个候选回复，再用奖励函数选出最优质的作为训练数据。
    过滤：从大型 SFT 数据集中挑选出回应质量高且提示多样性好的样本，形成精简的高质量数据集。
  核心：质量高于数量，利用此类策略的数据质量显著高于简单堆积大量普通数据质量。
全参数微调、参数高效微调（此两种策略可与任何训练方法结合）
  在执行对齐方法（如SFT）时，需要决定如何更新模型权重：
    全参数微调：修改所有参数。
      特点：显著提升性能。    缺点：耗用大量存储和计算资源。
    参数高效微调：如 LoRA（低秩适配），在每一层引入小的矩阵用来调整模型参数。
      特点：减少可训练参数的数量，节省显存，在硬件条件有限时更常用。    缺点：更新参数少，学习、遗忘有限。
  根据资源约束和性能要求来选择全参数微调还是参数高效微调。

资料链接：https://github.com/datawhalechina/Post-training-of-LLMs/blob/main/docs/chapter2/chapter2_1/%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BABasics%20of%20SFT.md
https://github.com/datawhalechina/Post-training-of-LLMs/blob/main/docs/chapter2/chapter2_2/Lesson_3.ipynb
https://github.com/datawhalechina/Post-training-of-LLMs/blob/main/docs/chapter2/chapter2_2/SFT_in_practice.ipynb
https://github.com/datawhalechina/Post-training-of-LLMs/blob/main/docs/chapter2/chapter2_2/requirements.txt



