课程总结
大语言模型训练后优化方法比较
  SFT：
    原理：通过最大化示例回答的概率来模仿目标响应模式。
    优势：1.实现简单；2.可快速启动模型新行为。
    劣势：可能降低训练数据未涵盖任务的性能。
  Online RL：
    原理：通过最大化回答的奖励函数进行优化。
    优势：提升模型能力的同时在未见任务上性能下降较少。
    劣势：1.实现复杂度最高；2.需要精心设计奖励函数。
  DPO:
    原理：通过对比学习鼓励优质回答/抑制劣质回答。
    优势：1.有效修正错误行为；2.针对性提升特定能力。
    劣势：1.可能出现过拟合；2.实现复杂度介于SFT和在线RL之间。
核心机制差异：
    OnlineRL：模型生成多组回答(R1,R2,R3)→获取奖励信号→在模型原生能力空间内调整权重→ 保持模型生成分布稳定性。
    SFT：要求模仿的示例答案可能与模型*自然生成分布**存在根本差异→ 强制模型偏离原始能力空间→权重发生非必要改变。












