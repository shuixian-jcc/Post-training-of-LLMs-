直接偏好优化基础理论(Basics of DPO)
概念
  可被视为一种从正面和负面回复中进行对比学习的方法，并且可从任何大语言模型开始训练
      （通常情况下建议使用指令微调大语言模型，可以通过整理标注员准备的对比数据来改变模型身份）
过程
  对指令准备至少两个回复，将理想回答标记为首选回答，将另一回答标记为次选回答，促使模型在回答相关问题时的答案趋近理想回答。
收集到此类对比数据后，就可以使用准备好的数据在该语言模型上执行直接偏好优化（DPO）。
  可理解为：
    在大语言模型上执行DPO后，将得到一个经过微调的LLM，希望它能从正向和负向样本中学习，它会尝试模仿偏好的样本。可以使用这种直接偏好优化方法改变模型的行为。
损失函数以及直接偏好优化（DPO）原理
  DPO旨在最小化对比损失，该损失对负面回复进行惩罚，并对正面回复进行奖励。
  DPO损失实际上是对重新参数化奖励模型的奖励差异的交叉熵损失
  损失（原文“让我们来看看这个DPO损失”~“在那里找到详细内容”）
最佳用例
  1.改变模型行为（最重要）:对模型响应进行小的修改时DPO更加有效。
      如：改变模型特性；使模型在多语言响应、指令遵循能力方面表现更好；改变模型一些与安全相关的响应等。
  2.提升模型能力：因为DPO能够同时看到好样本和坏样本的对比特性，所以在提升模型能力方面，它可能比监督微调（SFT）更有效。
特别是能使DPO实现对齐时，对提升能力来说效果甚至会更好。
数据整理原则（用于DPO）
  1.校对方法：从原始模型生成回复，将该回复作为一个主动样本，然后进行改进促使其成为一个正向回复。
    通过标记正向回复的这种方式，可以使用这种基于纠正的方法，自动创建大规模、高质量的对比数据，用于DPO的训练。
  2.在线或策略内DPO的一种特殊情况：从模型自身的分布中生成正向和负向示例。
    针对同一指令，从想要微调的LLM中生成多个回复,然后收集正负样本，并判断哪个回复更好/差（可使用奖励函数/人工判断完成）
  提示：避免过拟合（可理解为走捷径所导致LLM不稳定）

资料链接：
https://github.com/datawhalechina/Post-training-of-LLMs/blob/main/docs/chapter3/chapter3_1/直接偏好优化基础理论Basics%20of%20DPO.md
https://github.com/datawhalechina/Post-training-of-LLMs/tree/main/docs/chapter3/chapter3_2
















