一、
大语言模型训练分为预训练阶段（使用标记庞大、耗时长）和后训练阶段（耗时短成本低）
训练语言模型步骤：
随机模型——预训练—（得基础模型）—后训练1—（得指令/对话模型）—后训练2——（得定制化模型）
预训练目标：从各类数据源中学习知识
  基础模型：学习预测下一个词/标记
后训练1目标：从筛选数据中学习响应模式
  //数据包括对话数据、工具使用数据、智能体数据
  指令/对话模型：能对指令做出响应，执行更具体任务
后训练2目标：调整模型行为、增强特定能力
  定制化模型：具备特定的行为模式，专精于特定领域
二、
预训练方法（无监督学习）
可理解为：
从大规模无标记文本语料中提取标记（超两万亿个），最小化每个标记的负对数概念，然后给定前一个标记时后一个标记的负对数似然，然后给定前两个标记时后一个标记的概率（以此类推）
后训练方法（常用三种）
1.监督微调（SFT）：需创建有标记的提示-响应对数据集，通过所创建数据集训练模型，使模型学会遵守指令、使用工具
目标：让模型模仿学习所创建数据集间的映射关系
特点：最简单、最流行，适用于引入新行为或对模型进行重大调整，过程需要1000-10亿个标记
损失关键：只对响应标记进行训练
2.直接偏好优化（DPO）：需创建有提示及其对应优质/劣质响应的数据集，驱动模型学习
目标：使模型趋于优质响应并远离劣质响应
特点：过程需要1000-10亿个标记、使用构造性损失函数
3.在线强化学习（Online RL）：需提示集和奖励函数，通过提示让模型生成响应，再通过奖励函数对响应质量进行评分，根据奖励分数更新模型
  获取奖励函数方法：1.基于人类对响应质量的评判，训练与人类判断一致的评分函数（常用近端策略优化算法）
                  2.可验证奖励，验证响应的准确性及正确性（适用于具有客观正确性标准的指令） 
目标：通过模型自身所生成响应最大化奖励值
特点：过程需要1000-1000万（+）提示
三、
后训练的关键要素（3个）：
1.数据与算法的协同设计：所选择方法与数据结构的良好搭配
2.可靠高效的算法库：HuggingFace TRL实现大部分前述算法
             扩展：Open RLHF、veRL和Nemo RL是更精密、内存效率更高的库。
3.合适的评估体系：通过完善的评估方案评估模型在后训练前后的表现，确保后训练对模型是有效优化的
现有流行语言模型评估标准：
  对话机器人竞技场：基于人类偏好的聊天评估
  替代人类评判的LLM评估：AlpacaEval、MT Bench、Arena Hard
  指令模型静态基准：LiveCodeBench（热门代码基准）、AIME 2024/2025（高难度数学评估）
  知识与推理数据集：GPQA、MMLU Pro
  指令遵循评估：IFEval
  函数调用与智能体评估：BFCL、NexusBench、TauBench、ToolSandbox（后两者专注多工具使用场景）
四、
不是所有模型都要进行后训练，后训练更适用于需要严格遵守几十条甚至更多指令或提升特定能力时
因为后训练能可靠的改变模型行为并提升目标能力，但如果实施不当可能导致未训练能力退化

课程资料：
https://github.com/datawhalechina/Post-training-of-LLMs/blob/main/docs/chapter1/chapter1_1/%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8DIntroduction.md
https://github.com/datawhalechina/Post-training-of-LLMs/blob/main/docs/chapter1/chapter1_2/%E5%90%8E%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8DIntroduction%20to%20Post-training.md














